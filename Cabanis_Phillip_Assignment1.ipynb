{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 5s 114us/step - loss: 1.4829 - accuracy: 0.6231 - val_loss: 0.7584 - val_accuracy: 0.8286\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.6049 - accuracy: 0.8464 - val_loss: 0.4550 - val_accuracy: 0.8852\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.4398 - accuracy: 0.8801 - val_loss: 0.3710 - val_accuracy: 0.9019\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.3767 - accuracy: 0.8952 - val_loss: 0.3322 - val_accuracy: 0.9082\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.3415 - accuracy: 0.9025 - val_loss: 0.3055 - val_accuracy: 0.9147\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.3175 - accuracy: 0.9086 - val_loss: 0.2880 - val_accuracy: 0.9182\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 6s 116us/step - loss: 0.2989 - accuracy: 0.9137 - val_loss: 0.2727 - val_accuracy: 0.9224\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 5s 107us/step - loss: 0.2839 - accuracy: 0.9180 - val_loss: 0.2608 - val_accuracy: 0.9266\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 5s 113us/step - loss: 0.2714 - accuracy: 0.9217 - val_loss: 0.2505 - val_accuracy: 0.9298\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.2602 - accuracy: 0.9252 - val_loss: 0.2430 - val_accuracy: 0.9308\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 5s 114us/step - loss: 0.2501 - accuracy: 0.9285 - val_loss: 0.2341 - val_accuracy: 0.9335\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.2409 - accuracy: 0.9301 - val_loss: 0.2271 - val_accuracy: 0.9352\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 0.2325 - accuracy: 0.9334 - val_loss: 0.2227 - val_accuracy: 0.9367\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 0.2253 - accuracy: 0.9353 - val_loss: 0.2147 - val_accuracy: 0.9396\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 9s 197us/step - loss: 0.2181 - accuracy: 0.9375 - val_loss: 0.2082 - val_accuracy: 0.9411\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 9s 196us/step - loss: 0.2116 - accuracy: 0.9394 - val_loss: 0.2030 - val_accuracy: 0.9431\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 10s 201us/step - loss: 0.2055 - accuracy: 0.9414 - val_loss: 0.1981 - val_accuracy: 0.9445\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 8s 175us/step - loss: 0.1996 - accuracy: 0.9430 - val_loss: 0.1932 - val_accuracy: 0.9458\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 9s 180us/step - loss: 0.1941 - accuracy: 0.9432 - val_loss: 0.1894 - val_accuracy: 0.9467\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 8s 173us/step - loss: 0.1890 - accuracy: 0.9456 - val_loss: 0.1849 - val_accuracy: 0.9498\n",
      "10000/10000 [==============================] - 0s 41us/step\n",
      "Test score: 0.18599770209044217\n",
      "Test accuracy: 0.9463000297546387\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10\n",
    "OPTIMIZER = SGD() \n",
    "N_HIDDEN = 128 \n",
    "VALIDATION_SPLIT = 0.2\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() #X_train is 60000 rows of 28x28 values\n",
    "RESHAPED = 784 \n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers # 10 outputs # final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, \n",
    "                    validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/250\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 1.7374 - accuracy: 0.4479 - val_loss: 0.9218 - val_accuracy: 0.8137\n",
      "Epoch 2/250\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.9255 - accuracy: 0.7173 - val_loss: 0.5297 - val_accuracy: 0.8680\n",
      "Epoch 3/250\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.6925 - accuracy: 0.7883 - val_loss: 0.4229 - val_accuracy: 0.8910\n",
      "Epoch 4/250\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.5885 - accuracy: 0.8229 - val_loss: 0.3706 - val_accuracy: 0.8986\n",
      "Epoch 5/250\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.5269 - accuracy: 0.8414 - val_loss: 0.3364 - val_accuracy: 0.9046\n",
      "Epoch 6/250\n",
      "48000/48000 [==============================] - 6s 128us/step - loss: 0.4875 - accuracy: 0.8556 - val_loss: 0.3131 - val_accuracy: 0.9106\n",
      "Epoch 7/250\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.4521 - accuracy: 0.8655 - val_loss: 0.2939 - val_accuracy: 0.9144\n",
      "Epoch 8/250\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.4229 - accuracy: 0.8750 - val_loss: 0.2788 - val_accuracy: 0.9191\n",
      "Epoch 9/250\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.4066 - accuracy: 0.8792 - val_loss: 0.2660 - val_accuracy: 0.9218\n",
      "Epoch 10/250\n",
      "48000/48000 [==============================] - 5s 105us/step - loss: 0.3866 - accuracy: 0.8863 - val_loss: 0.2544 - val_accuracy: 0.9258\n",
      "Epoch 11/250\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.3723 - accuracy: 0.8892 - val_loss: 0.2443 - val_accuracy: 0.9285\n",
      "Epoch 12/250\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.3539 - accuracy: 0.8959 - val_loss: 0.2350 - val_accuracy: 0.9319\n",
      "Epoch 13/250\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.3436 - accuracy: 0.8991 - val_loss: 0.2275 - val_accuracy: 0.9331\n",
      "Epoch 14/250\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 0.3308 - accuracy: 0.9023 - val_loss: 0.2209 - val_accuracy: 0.9350\n",
      "Epoch 15/250\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.3256 - accuracy: 0.9037 - val_loss: 0.2153 - val_accuracy: 0.9361\n",
      "Epoch 16/250\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.3125 - accuracy: 0.9076 - val_loss: 0.2082 - val_accuracy: 0.9383\n",
      "Epoch 17/250\n",
      "48000/48000 [==============================] - 5s 104us/step - loss: 0.3039 - accuracy: 0.9117 - val_loss: 0.2033 - val_accuracy: 0.9409\n",
      "Epoch 18/250\n",
      "48000/48000 [==============================] - 6s 117us/step - loss: 0.2988 - accuracy: 0.9121 - val_loss: 0.1981 - val_accuracy: 0.9423\n",
      "Epoch 19/250\n",
      "48000/48000 [==============================] - 6s 115us/step - loss: 0.2881 - accuracy: 0.9156 - val_loss: 0.1923 - val_accuracy: 0.9435\n",
      "Epoch 20/250\n",
      "48000/48000 [==============================] - 5s 113us/step - loss: 0.2825 - accuracy: 0.9170 - val_loss: 0.1882 - val_accuracy: 0.9449\n",
      "Epoch 21/250\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.2773 - accuracy: 0.9185 - val_loss: 0.1850 - val_accuracy: 0.9457\n",
      "Epoch 22/250\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.2719 - accuracy: 0.9214 - val_loss: 0.1801 - val_accuracy: 0.9470\n",
      "Epoch 23/250\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.2646 - accuracy: 0.9219 - val_loss: 0.1770 - val_accuracy: 0.9483\n",
      "Epoch 24/250\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.2610 - accuracy: 0.9229 - val_loss: 0.1741 - val_accuracy: 0.9490\n",
      "Epoch 25/250\n",
      "48000/48000 [==============================] - 6s 119us/step - loss: 0.2570 - accuracy: 0.9244 - val_loss: 0.1707 - val_accuracy: 0.9504\n",
      "Epoch 26/250\n",
      "48000/48000 [==============================] - 6s 126us/step - loss: 0.2503 - accuracy: 0.9261 - val_loss: 0.1673 - val_accuracy: 0.9510\n",
      "Epoch 27/250\n",
      "48000/48000 [==============================] - 7s 149us/step - loss: 0.2459 - accuracy: 0.9268 - val_loss: 0.1649 - val_accuracy: 0.9521\n",
      "Epoch 28/250\n",
      "48000/48000 [==============================] - 7s 141us/step - loss: 0.2402 - accuracy: 0.9292 - val_loss: 0.1619 - val_accuracy: 0.9527\n",
      "Epoch 29/250\n",
      "48000/48000 [==============================] - 6s 135us/step - loss: 0.2355 - accuracy: 0.9306 - val_loss: 0.1597 - val_accuracy: 0.9536\n",
      "Epoch 30/250\n",
      "48000/48000 [==============================] - 7s 136us/step - loss: 0.2358 - accuracy: 0.9301 - val_loss: 0.1573 - val_accuracy: 0.9539\n",
      "Epoch 31/250\n",
      "48000/48000 [==============================] - 6s 134us/step - loss: 0.2294 - accuracy: 0.9326 - val_loss: 0.1550 - val_accuracy: 0.9551\n",
      "Epoch 32/250\n",
      "48000/48000 [==============================] - 7s 137us/step - loss: 0.2235 - accuracy: 0.9331 - val_loss: 0.1531 - val_accuracy: 0.9562\n",
      "Epoch 33/250\n",
      "48000/48000 [==============================] - 7s 156us/step - loss: 0.2253 - accuracy: 0.9344 - val_loss: 0.1506 - val_accuracy: 0.9562\n",
      "Epoch 34/250\n",
      "48000/48000 [==============================] - 7s 136us/step - loss: 0.2201 - accuracy: 0.9348 - val_loss: 0.1489 - val_accuracy: 0.9566\n",
      "Epoch 35/250\n",
      "48000/48000 [==============================] - 7s 138us/step - loss: 0.2172 - accuracy: 0.9367 - val_loss: 0.1471 - val_accuracy: 0.9571\n",
      "Epoch 36/250\n",
      "48000/48000 [==============================] - 7s 139us/step - loss: 0.2150 - accuracy: 0.9369 - val_loss: 0.1455 - val_accuracy: 0.9580\n",
      "Epoch 37/250\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.2116 - accuracy: 0.9376 - val_loss: 0.1436 - val_accuracy: 0.9580\n",
      "Epoch 38/250\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.2063 - accuracy: 0.9398 - val_loss: 0.1414 - val_accuracy: 0.9588\n",
      "Epoch 39/250\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.2045 - accuracy: 0.9398 - val_loss: 0.1399 - val_accuracy: 0.9597\n",
      "Epoch 40/250\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.2031 - accuracy: 0.9401 - val_loss: 0.1391 - val_accuracy: 0.9606\n",
      "Epoch 41/250\n",
      "48000/48000 [==============================] - 3s 73us/step - loss: 0.1994 - accuracy: 0.9412 - val_loss: 0.1376 - val_accuracy: 0.9604\n",
      "Epoch 42/250\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.1964 - accuracy: 0.9429 - val_loss: 0.1356 - val_accuracy: 0.9610\n",
      "Epoch 43/250\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.1945 - accuracy: 0.9432 - val_loss: 0.1340 - val_accuracy: 0.9620\n",
      "Epoch 44/250\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.1929 - accuracy: 0.9426 - val_loss: 0.1335 - val_accuracy: 0.9625\n",
      "Epoch 45/250\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.1889 - accuracy: 0.9437 - val_loss: 0.1318 - val_accuracy: 0.9621\n",
      "Epoch 46/250\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.1854 - accuracy: 0.9452 - val_loss: 0.1303 - val_accuracy: 0.9627\n",
      "Epoch 47/250\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.1850 - accuracy: 0.9455 - val_loss: 0.1289 - val_accuracy: 0.9632\n",
      "Epoch 48/250\n",
      "48000/48000 [==============================] - 4s 79us/step - loss: 0.1820 - accuracy: 0.9471 - val_loss: 0.1290 - val_accuracy: 0.9626\n",
      "Epoch 49/250\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.1822 - accuracy: 0.9461 - val_loss: 0.1277 - val_accuracy: 0.9632\n",
      "Epoch 50/250\n",
      "48000/48000 [==============================] - 4s 87us/step - loss: 0.1798 - accuracy: 0.9469 - val_loss: 0.1267 - val_accuracy: 0.9638\n",
      "Epoch 51/250\n",
      "48000/48000 [==============================] - 4s 92us/step - loss: 0.1770 - accuracy: 0.9467 - val_loss: 0.1254 - val_accuracy: 0.9638\n",
      "Epoch 52/250\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.1768 - accuracy: 0.9472 - val_loss: 0.1248 - val_accuracy: 0.9642\n",
      "Epoch 53/250\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.1732 - accuracy: 0.9490 - val_loss: 0.1237 - val_accuracy: 0.9647\n",
      "Epoch 54/250\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.1742 - accuracy: 0.9493 - val_loss: 0.1219 - val_accuracy: 0.9648\n",
      "Epoch 55/250\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.1704 - accuracy: 0.9492 - val_loss: 0.1215 - val_accuracy: 0.9652\n",
      "Epoch 56/250\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.1699 - accuracy: 0.9488 - val_loss: 0.1207 - val_accuracy: 0.9654\n",
      "Epoch 57/250\n",
      "48000/48000 [==============================] - 4s 87us/step - loss: 0.1691 - accuracy: 0.9510 - val_loss: 0.1194 - val_accuracy: 0.9655\n",
      "Epoch 58/250\n",
      "48000/48000 [==============================] - 4s 92us/step - loss: 0.1654 - accuracy: 0.9504 - val_loss: 0.1187 - val_accuracy: 0.9659\n",
      "Epoch 59/250\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.1648 - accuracy: 0.9509 - val_loss: 0.1176 - val_accuracy: 0.9663\n",
      "Epoch 60/250\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.1623 - accuracy: 0.9511 - val_loss: 0.1168 - val_accuracy: 0.9663\n",
      "Epoch 61/250\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.1601 - accuracy: 0.9526 - val_loss: 0.1161 - val_accuracy: 0.9667\n",
      "Epoch 62/250\n",
      "48000/48000 [==============================] - 7s 153us/step - loss: 0.1608 - accuracy: 0.9526 - val_loss: 0.1156 - val_accuracy: 0.9665\n",
      "Epoch 63/250\n",
      "48000/48000 [==============================] - 6s 116us/step - loss: 0.1562 - accuracy: 0.9540 - val_loss: 0.1146 - val_accuracy: 0.9668\n",
      "Epoch 64/250\n",
      "48000/48000 [==============================] - 5s 114us/step - loss: 0.1577 - accuracy: 0.9531 - val_loss: 0.1145 - val_accuracy: 0.9663\n",
      "Epoch 65/250\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.1535 - accuracy: 0.9547 - val_loss: 0.1134 - val_accuracy: 0.9667\n",
      "Epoch 66/250\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.1529 - accuracy: 0.9553 - val_loss: 0.1128 - val_accuracy: 0.9668\n",
      "Epoch 67/250\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.1540 - accuracy: 0.9531 - val_loss: 0.1122 - val_accuracy: 0.9671\n",
      "Epoch 68/250\n",
      "48000/48000 [==============================] - 6s 116us/step - loss: 0.1508 - accuracy: 0.9542 - val_loss: 0.1108 - val_accuracy: 0.9670\n",
      "Epoch 69/250\n",
      "48000/48000 [==============================] - 9s 178us/step - loss: 0.1502 - accuracy: 0.9551 - val_loss: 0.1106 - val_accuracy: 0.9679\n",
      "Epoch 70/250\n",
      "48000/48000 [==============================] - 9s 177us/step - loss: 0.1498 - accuracy: 0.9557 - val_loss: 0.1098 - val_accuracy: 0.9675\n",
      "Epoch 71/250\n",
      "48000/48000 [==============================] - 9s 189us/step - loss: 0.1472 - accuracy: 0.9565 - val_loss: 0.1090 - val_accuracy: 0.9682\n",
      "Epoch 72/250\n",
      "48000/48000 [==============================] - 10s 198us/step - loss: 0.1484 - accuracy: 0.9553 - val_loss: 0.1084 - val_accuracy: 0.9683\n",
      "Epoch 73/250\n",
      "48000/48000 [==============================] - 8s 168us/step - loss: 0.1457 - accuracy: 0.9566 - val_loss: 0.1081 - val_accuracy: 0.9683\n",
      "Epoch 74/250\n",
      "48000/48000 [==============================] - 9s 180us/step - loss: 0.1430 - accuracy: 0.9572 - val_loss: 0.1075 - val_accuracy: 0.9687\n",
      "Epoch 75/250\n",
      "48000/48000 [==============================] - 7s 147us/step - loss: 0.1418 - accuracy: 0.9577 - val_loss: 0.1074 - val_accuracy: 0.9685\n",
      "Epoch 76/250\n",
      "48000/48000 [==============================] - 6s 121us/step - loss: 0.1438 - accuracy: 0.9572 - val_loss: 0.1064 - val_accuracy: 0.9690\n",
      "Epoch 77/250\n",
      "48000/48000 [==============================] - 6s 121us/step - loss: 0.1416 - accuracy: 0.9572 - val_loss: 0.1065 - val_accuracy: 0.9688\n",
      "Epoch 78/250\n",
      "48000/48000 [==============================] - 7s 137us/step - loss: 0.1387 - accuracy: 0.9590 - val_loss: 0.1052 - val_accuracy: 0.9691\n",
      "Epoch 79/250\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.1371 - accuracy: 0.9593 - val_loss: 0.1051 - val_accuracy: 0.9694\n",
      "Epoch 80/250\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.1382 - accuracy: 0.9586 - val_loss: 0.1049 - val_accuracy: 0.9687\n",
      "Epoch 81/250\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 0.1365 - accuracy: 0.9598 - val_loss: 0.1042 - val_accuracy: 0.9691\n",
      "Epoch 82/250\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 0.1349 - accuracy: 0.9596 - val_loss: 0.1030 - val_accuracy: 0.9694\n",
      "Epoch 83/250\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.1345 - accuracy: 0.9594 - val_loss: 0.1027 - val_accuracy: 0.9706\n",
      "Epoch 84/250\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.1340 - accuracy: 0.9594 - val_loss: 0.1025 - val_accuracy: 0.9698\n",
      "Epoch 85/250\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.1329 - accuracy: 0.9609 - val_loss: 0.1018 - val_accuracy: 0.9704\n",
      "Epoch 86/250\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.1327 - accuracy: 0.9596 - val_loss: 0.1019 - val_accuracy: 0.9706\n",
      "Epoch 87/250\n",
      "33536/48000 [===================>..........] - ETA: 1s - loss: 0.1312 - accuracy: 0.9612"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b2d7441925e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNB_EPOCH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVERBOSE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m                     validation_split=VALIDATION_SPLIT) \n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVERBOSE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test score:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "# network and training \n",
    "NB_EPOCH = 250 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10\n",
    "OPTIMIZER = SGD() \n",
    "N_HIDDEN = 128 \n",
    "VALIDATION_SPLIT = 0.2\n",
    "DROPOUT = 0.3\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() #X_train is 60000 rows of 28x28 values\n",
    "RESHAPED = 784 \n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers # 10 outputs # final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, \n",
    "                    validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.6706 - accuracy: 0.7903 - val_loss: 0.2238 - val_accuracy: 0.9359\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.3389 - accuracy: 0.9028 - val_loss: 0.1710 - val_accuracy: 0.9506\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.2733 - accuracy: 0.9222 - val_loss: 0.1550 - val_accuracy: 0.9563\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 7s 143us/step - loss: 0.2451 - accuracy: 0.9315 - val_loss: 0.1372 - val_accuracy: 0.9619\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.2210 - accuracy: 0.9374 - val_loss: 0.1287 - val_accuracy: 0.9647\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 7s 148us/step - loss: 0.2139 - accuracy: 0.9406 - val_loss: 0.1328 - val_accuracy: 0.9632\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 7s 149us/step - loss: 0.1998 - accuracy: 0.9448 - val_loss: 0.1298 - val_accuracy: 0.9668\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 6s 135us/step - loss: 0.1962 - accuracy: 0.9453 - val_loss: 0.1208 - val_accuracy: 0.9682\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 133us/step - loss: 0.1937 - accuracy: 0.9479 - val_loss: 0.1196 - val_accuracy: 0.9710\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 7s 146us/step - loss: 0.1824 - accuracy: 0.9510 - val_loss: 0.1185 - val_accuracy: 0.9694\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 7s 148us/step - loss: 0.1717 - accuracy: 0.9519 - val_loss: 0.1189 - val_accuracy: 0.9720\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.1728 - accuracy: 0.9533 - val_loss: 0.1200 - val_accuracy: 0.9704\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.1699 - accuracy: 0.9539 - val_loss: 0.1234 - val_accuracy: 0.9714\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 7s 137us/step - loss: 0.1644 - accuracy: 0.9550 - val_loss: 0.1233 - val_accuracy: 0.9726\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.1701 - accuracy: 0.9549 - val_loss: 0.1252 - val_accuracy: 0.9731\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 7s 147us/step - loss: 0.1639 - accuracy: 0.9572 - val_loss: 0.1204 - val_accuracy: 0.9722\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.1654 - accuracy: 0.9564 - val_loss: 0.1213 - val_accuracy: 0.9723\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 7s 146us/step - loss: 0.1647 - accuracy: 0.9561 - val_loss: 0.1294 - val_accuracy: 0.9737\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 7s 145us/step - loss: 0.1610 - accuracy: 0.9576 - val_loss: 0.1292 - val_accuracy: 0.9736\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.1620 - accuracy: 0.9584 - val_loss: 0.1298 - val_accuracy: 0.9727\n",
      "10000/10000 [==============================] - 0s 41us/step\n",
      "Test score: 0.12601815673169914\n",
      "Test accuracy: 0.973800003528595\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10\n",
    "OPTIMIZER = RMSprop()  #Changed optimizer\n",
    "N_HIDDEN = 128 \n",
    "VALIDATION_SPLIT = 0.2\n",
    "DROPOUT = 0.5 #increased dropout rate to  see results\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() #X_train is 60000 rows of 28x28 values\n",
    "RESHAPED = 784 \n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers # 10 outputs # final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, \n",
    "                    validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 180)               141300    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 180)               32580     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                1810      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 175,690\n",
      "Trainable params: 175,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 7s 144us/step - loss: 0.4889 - accuracy: 0.8515 - val_loss: 0.1771 - val_accuracy: 0.9463\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.2054 - accuracy: 0.9389 - val_loss: 0.1267 - val_accuracy: 0.9619\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 7s 151us/step - loss: 0.1547 - accuracy: 0.9538 - val_loss: 0.1066 - val_accuracy: 0.9675\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 7s 154us/step - loss: 0.1254 - accuracy: 0.9624 - val_loss: 0.0966 - val_accuracy: 0.9703\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 7s 153us/step - loss: 0.1077 - accuracy: 0.9668 - val_loss: 0.0884 - val_accuracy: 0.9734\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 7s 152us/step - loss: 0.0950 - accuracy: 0.9702 - val_loss: 0.0801 - val_accuracy: 0.9758\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 8s 158us/step - loss: 0.0854 - accuracy: 0.9737 - val_loss: 0.0852 - val_accuracy: 0.9762\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 8s 158us/step - loss: 0.0750 - accuracy: 0.9762 - val_loss: 0.0810 - val_accuracy: 0.9758\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 7s 156us/step - loss: 0.0712 - accuracy: 0.9774 - val_loss: 0.0816 - val_accuracy: 0.9766\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 8s 157us/step - loss: 0.0644 - accuracy: 0.9791 - val_loss: 0.0793 - val_accuracy: 0.9757\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 8s 168us/step - loss: 0.0585 - accuracy: 0.9813 - val_loss: 0.0742 - val_accuracy: 0.9786\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 8s 169us/step - loss: 0.0585 - accuracy: 0.9815 - val_loss: 0.0719 - val_accuracy: 0.9801\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 7s 155us/step - loss: 0.0495 - accuracy: 0.9840 - val_loss: 0.0760 - val_accuracy: 0.9794\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 7s 151us/step - loss: 0.0481 - accuracy: 0.9842 - val_loss: 0.0782 - val_accuracy: 0.9774\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 7s 150us/step - loss: 0.0476 - accuracy: 0.9846 - val_loss: 0.0743 - val_accuracy: 0.9794\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 8s 159us/step - loss: 0.0443 - accuracy: 0.9852 - val_loss: 0.0746 - val_accuracy: 0.9801\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 7s 144us/step - loss: 0.0421 - accuracy: 0.9866 - val_loss: 0.0775 - val_accuracy: 0.9792\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 7s 156us/step - loss: 0.0407 - accuracy: 0.9865 - val_loss: 0.0721 - val_accuracy: 0.9802\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 8s 159us/step - loss: 0.0396 - accuracy: 0.9864 - val_loss: 0.0777 - val_accuracy: 0.9787\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 7s 156us/step - loss: 0.0363 - accuracy: 0.9881 - val_loss: 0.0785 - val_accuracy: 0.9803\n",
      "10000/10000 [==============================] - 1s 53us/step\n",
      "Test score: 0.07188344250550509\n",
      "Test accuracy: 0.9811999797821045\n"
     ]
    }
   ],
   "source": [
    "#### from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "# network and training \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 180 # Altered batch size\n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10\n",
    "OPTIMIZER = Adam() # Changed Optimizer\n",
    "N_HIDDEN = 180 # Also altered N_hidden\n",
    "VALIDATION_SPLIT = 0.2\n",
    "DROPOUT = 0.3\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() #X_train is 60000 rows of 28x28 values\n",
    "RESHAPED = 784 \n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers # 10 outputs # final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, \n",
    "                    validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second cell, I altered the number of epochs that are run just like in the book. This gave the program more time to learn, and resulted in a much higher accuracy rate.\n",
    "\n",
    "In the third trial, I changed optimizers from SGD, to RMSprop and increased the dropout from 0.3 to 0.5. This resulted an interesting find. At epoch #15 the program reached a peak of 95.50 and then dropped slightly. Then went up slightly, but never over 95. it continued to do this until finished. The function must have peaked slightly and stopped learning as fast. I believe with more epochs, that the growth would slowly increase. However, this would take considerable resources for a very small result. \n",
    "\n",
    "For the final trial, I again changed the optimizer to Adam, and changed the BATCH_SIZE and N_HIDDEN variable from 128 to 180. The first epoch had a very high accuracy of 84% and continued to increase fast until it hit 97% then the program finished with a very high 98.81%. The sudden jump from 84% to 95% in the first 2 epochs can be attributed to the better optimizer as well as the increased batch size. I also read that increasing hiddens can increase accuracy by adding more layers to the network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
